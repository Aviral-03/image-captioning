{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Structure\n",
    "\n",
    "We want the data as a dictionary with key-value pairs, where the key is the file path and the value are the captions. The file paths are obtained from the Flicker8k dataset, and the captions are retrieved accordingly from the Flicker8k.token.txt and Flicker8k.lemma.token.txt files.\n",
    "\n",
    "## Folder Structure\n",
    "```\n",
    "Flicker8k_Dataset/\n",
    "├── Images/\n",
    "│   ├── image_files\n",
    "├── Flicker8k.token.txt\n",
    "├── Flicker8k.lemma.token.txt \n",
    "```\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T22:45:23.843682Z",
     "start_time": "2024-09-07T22:45:23.840487Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "import torchtext\n",
    "from torch.utils.data import Dataset\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T22:45:53.754027Z",
     "start_time": "2024-09-07T22:45:53.747040Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LoadFlickerData(Dataset):\n",
    "    def __init__(self, dataset_path, max_seq_length, tokenizer):\n",
    "        self.path = dataset_path  # Path to the dataset file\n",
    "        self.data = None  # {image_id/image_path: [tokenized_captions1, tokenized_captions2, ..., lemmatized_captions]}\n",
    "        self.tokenizer = tokenizer  # Tokenizer function\n",
    "        self.vocab = None\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.load_data()\n",
    "\n",
    "    def load_data(self):\n",
    "        path_to_tokens = self.path + '/Flickr8k.token.txt'\n",
    "        path_to_lemmas = self.path + '/Flickr8k.lemma.token.txt'\n",
    "        self.data = defaultdict(list)\n",
    "\n",
    "        # Read the data from the files\n",
    "        with open(path_to_tokens, 'r') as f:\n",
    "            for line in f:\n",
    "                image_id, caption = line.strip().split('\\t')\n",
    "                image_id = image_id.split('.')[0]  # Get image id before .jpg\n",
    "                self.data[image_id].append(caption)\n",
    "        with open(path_to_lemmas, 'r') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                image_id, caption = line.strip().split('\\t')\n",
    "                image_id = image_id.split('.')[0]  # Get image id before .jpg\n",
    "                self.data[image_id].append(caption)\n",
    "\n",
    "        # Create dataframes for the data\n",
    "        dataframe = pd.DataFrame(self.data.items(), columns=['image_id', 'captions'])\n",
    "        dataframe['image_path'] = dataframe['image_id'].apply(lambda x: self.path + '/Images/' + x + '.jpg')\n",
    "        dataframe['captions'] = dataframe['captions'].apply(lambda x: x[0])\n",
    "        self.data = dataframe.set_index('image_id').to_dict(orient='index')\n",
    "\n",
    "    def __preprocess_image__(self, image_path):\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.resize(image, (224, 224))\n",
    "        image = np.transpose(image, (2, 0, 1))\n",
    "        image = torch.tensor(image, dtype=torch.float32)\n",
    "        return image\n",
    "\n",
    "    def __preprocess_caption__(self, image_data):\n",
    "        counter = Counter()\n",
    "        for image_id, captions in image_data.items():\n",
    "            for caption in captions:\n",
    "                counter.update(self.tokenizer(caption))\n",
    "\n",
    "        vocab = torchtext.vocab.Vocab(counter, specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
    "\n",
    "        tokens_arrays = []\n",
    "        attention_masks = []\n",
    "\n",
    "        for image_id, captions in image_data.items():\n",
    "            for caption in captions:\n",
    "                tokens = [vocab[token] for token in self.tokenizer(caption)]\n",
    "                tokens = [vocab['<bos>']] + tokens + [vocab['<eos>']]\n",
    "\n",
    "                # Create attention mask\n",
    "                mask = torch.ones(self.max_seq_length, dtype=torch.bool)\n",
    "\n",
    "                # Padding or truncating the tokens to match max_seq_length\n",
    "                if len(tokens) <= self.max_seq_length:\n",
    "                    pad_starts = len(tokens)  # Record the length before padding\n",
    "                    tokens = tokens + [vocab['<pad>']] * (self.max_seq_length - len(tokens))\n",
    "                    mask[pad_starts:] = False  # Set the padded part to 0 in the attention mask\n",
    "                else:\n",
    "                    tokens = tokens[:self.max_seq_length - 1] + [vocab['<eos>']]\n",
    "\n",
    "                # Append the processed tokens and attention mask\n",
    "                tokens_arrays.append(tokens)\n",
    "                attention_masks.append(mask)\n",
    "\n",
    "        return vocab, tokens_arrays, attention_masks\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        # Tokenize and process the captions\n",
    "        vocab, tokens_arrays, attention_masks = self.__preprocess_caption__(self.data)\n",
    "        image = self.__preprocess_image__(self.data[item]['image_path'])\n",
    "\n",
    "        # Get the relevant tokenized captions and attention masks for this item\n",
    "        out_dict = {\n",
    "            \"image\": image,\n",
    "            \"caption_tokens\": torch.tensor(tokens_arrays[item], dtype=torch.long),\n",
    "            \"captions\": self.data[item]['captions'],\n",
    "            \"attention_mask\": torch.tensor(attention_masks[item], dtype=torch.bool)\n",
    "        }\n",
    "\n",
    "        return out_dict\n"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T22:45:54.908907Z",
     "start_time": "2024-09-07T22:45:54.803436Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = LoadFlickerData('/Volumes/Aviral/Flicker8k_Dataset', lambda x: x.split())\n",
    "data.load_data()\n",
    "sample_data = data.get_data()"
   ],
   "outputs": [],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
