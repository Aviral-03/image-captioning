{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ImageClassificationModel(torch.nn.Module):\n",
    "    def __init__(self, cnn_model, image_size, vocab_size, max_seq_length, dropout=0.1, embed_size=512):\n",
    "        super(ImageClassificationModel, self).__init__()\n",
    "        self.encoder = EncoderCNN(cnn_model)\n",
    "        self.decoder = DecoderRNN(cnn_model, image_size, vocab_size, embed_size, max_seq_length, dropout=dropout)\n",
    "        \n",
    "    def forward(self, image, captions):\n",
    "        image_features = self.encoder(image)\n",
    "        output = self.decoder(image_features, captions)\n",
    "        return output\n",
    "    \n",
    "    def generate_caption(self, image, max_seq_length):\n",
    "        image_features = self.encoder(image)\n",
    "        batch_size = image_features.shape[0]\n",
    "        tgt = torch.ones(batch_size, 1).long().to(image.device)\n",
    "        for i in range(max_seq_length):\n",
    "            output = self.decoder(image_features, tgt)\n",
    "            output = torch.argmax(output, dim=2)\n",
    "            tgt = torch.cat((tgt, output[:, -1].unsqueeze(1)), dim=1)\n",
    "        return tgt\n",
    "    \n",
    "    def generate_caption_beam_search(self, image, max_seq_length, beam_size=3):\n",
    "        image_features = self.encoder(image)\n",
    "        batch_size = image_features.shape[0]\n",
    "        tgt = torch.ones(batch_size, 1).long().to(image.device)\n",
    "        for i in range(max_seq_length):\n",
    "            output = self.decoder(image_features, tgt)\n",
    "            output = torch.argmax(output, dim=2)\n",
    "            tgt = torch.cat((tgt, output[:, -1].unsqueeze(1)), dim=1)\n",
    "        return tgt\n",
    "    \n",
    "    def generate_caption_greedy_search(self, image, max_seq_length):\n",
    "        image_features = self.encoder(image)\n",
    "        batch_size = image_features.shape[0]\n",
    "        tgt = torch.ones(batch_size, 1).long().to(image.device)\n",
    "        for i in range(max_seq_length):\n",
    "            output = self.decoder(image_features, tgt)\n",
    "            output = torch.argmax(output, dim=2)\n",
    "            tgt = torch.cat((tgt, output[:, -1].unsqueeze(1)), dim=1)\n",
    "        return tgt\n",
    "    \n",
    "    def generate_caption_sampling(self, image, max_seq_length):\n",
    "        image_features = self.encoder(image)\n",
    "        batch_size = image_features.shape[0]\n",
    "        tgt = torch.ones(batch_size, 1).long().to(image.device)\n",
    "        for i in range(max_seq_length):\n",
    "            output = self.decoder(image_features, tgt)\n",
    "            output = torch.argmax(output, dim=2)\n",
    "            tgt = torch.cat((tgt, output[:, -1].unsqueeze(1)), dim=1)\n",
    "        return tgt\n",
    "    \n",
    "    \n",
    "    # class DecoderRNN(torch.nn.Module):\n",
    "#     def __init__(self, cnn_model, image_size, vocab_size, embed_size, max_seq_length, dropout=0.1, num_layers=6):\n",
    "#         super(DecoderRNN, self).__init__()\n",
    "#         self.token_embed = torch.nn.Embedding(vocab_size, embed_size) # Token embedding\n",
    "#         self.position_embed = PositionalEncoding(embed_size, dropout=dropout, max_len=max_seq_length)\n",
    "#         self.decoder = torch.nn.Transformer(d_model=embed_size, nhead=8, num_encoder_layers=num_layers, num_decoder_layers=num_layers, dropout=dropout)\n",
    "#         self.fc = torch.nn.Linear(embed_size, vocab_size) # Fully connected layer to output the predicted token\n",
    "#         self.dropout = torch.nn.Dropout(dropout)\n",
    "#         \n",
    "#         # Image features from CNN Encoder\n",
    "#         self.encoder = cnn_model\n",
    "#         \n",
    "#         \n",
    "#     def forward(self, image, captions, tgt_mask=None):\n",
    "#         # image = self.embed(image)\n",
    "#         batch_size, seq_len = captions.shape[0], captions.shape[1]\n",
    "#         \n",
    "#         # Get Image Features from Encoder\n",
    "#         with torch.no_grad():\n",
    "#             image_features = self.encoder(image)\n",
    "#         image_features = image_features.view(image_features.shape[0], -1) # [seq_len, batch_size, embed_size]\n",
    "#         \n",
    "#         # Decoder\n",
    "#         tgt = self.token_embed(captions) * torch.sqrt(torch.tensor([self.token_embed.embedding_dim]).to(image.device))\n",
    "#         # Add Positional Encoding to the token embeddings\n",
    "#         tgt = self.position_embed(tgt)\n",
    "#         tgt = tgt.permute(1, 0, 2) # [seq_len, batch_size, embed_size]\n",
    "#         encoder_memory = self.encoder(image)\n",
    "#         tgt_mask = self.transformer.generate_square_subsequent_mask(tgt.size(0)).to(tgt.device)\n",
    "#         output = self.decoder(tgt, encoder_memory, tgt_mask=tgt_mask)\n",
    "#         output = output.permute(1, 0, 2) # [batch_size, seq_len, embed_size]\n",
    "#         output = self.fc(output)\n",
    "#         return output, encoder_memory\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
