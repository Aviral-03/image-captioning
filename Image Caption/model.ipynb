{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### d_model: the number of expected features in the input (required).\n",
    "#### nhead: the number of heads in the multiheadattention models (required).\n",
    "#### dropout: the dropout value (default=0.1).\n",
    "#### activation: the activation function of the intermediate layer, can be a string (\"relu\" or \"gelu\") or a unary callable. Default: relu"
   ],
   "id": "bfaa85bbc486abf6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from torchvision import models\n",
    "from torch.nn import MultiheadAttention\n",
    "from transformers.models.deprecated.transfo_xl.modeling_transfo_xl import PositionwiseFF"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Encoder architecture for image features\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        \"\"\"\n",
    "        :param embed_size: size of the embedding vector is used to represent the image features in vector form\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__() # Load the pretrained model\n",
    "        image_encoder = models.vgg16(pretrained=True)\n",
    "        # freeze all VGG parameters so we don't backprop through them since they are pre-trained\n",
    "        for param in image_encoder.parameters():\n",
    "            param.requires_grad_(False) \n",
    "        \n",
    "        # The last few layers in VGG16 are fully connected layers used for classification, but in an encoder, we're interested in extracting high-level features from the image\n",
    "        modules = list(image_encoder.features)[:-1] \n",
    "        \n",
    "        # Since we have updated the model architecture, we are updating the image_encoder model\n",
    "        self.image_encoder = torch.nn.Sequential(*modules) \n",
    "        \n",
    "        # Note: ensure the output shape of self.image_encoder matches images_encoder_classifier[0].in_features\n",
    "        \n",
    "        # Add a linear layer to transform the features into the desired dimension\n",
    "        self.embed = torch.nn.Linear(image_encoder.classifier[0].in_features, embed_size) # Add a linear layer to transform the features into the desired dimension\n",
    "        \n",
    "    def forward(self, image):\n",
    "        \"\"\"\n",
    "        :param image: Tensor, shape [batch_size, 3, 224, 224]\n",
    "        :return: Tensor, shape [batch_size, embed_size]\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            features = self.image_encoder(image)\n",
    "            \n",
    "        # Resize the features to have the same size as the input to the decoder\n",
    "        features = features.view(features.size(0), -1)\n",
    "        features = self.embed(features)\n",
    "        \n",
    "        return features\n",
    "                "
   ],
   "id": "92cfcc362b8e9b31"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Positional Encoding [credit: https://medium.com/@hunter-j-phillips/positional-encoding-7a93db4109e6]\n",
    "class PositionalEncoding(torch.nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0) # [1, max_len, d_model]\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Tensor, shape [seq_len, batch_size, d_model or embed_size]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:, :x.size(1)].requires_grad_(False)\n",
    "        return self.dropout(x)"
   ],
   "id": "24b07d69e0a8631a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Decoder Architecture\n",
    "class TransformerDecoderLayer(torch.nn.Module):\n",
    "    def __init__(self, d_model: int, n_head: int, d_ffn: int, dropout: float=0.1):\n",
    "        \"\"\"\n",
    "        Decoder Layer Transformer Architecture\n",
    "        :param d_model: dimension of the vectors flowing through the model (generally it is d_model = embed_size = 512)\n",
    "        :param n_head: number of heads in the multi-head attention models (generally n_head = 8) \n",
    "        :param d_ffn: dimension of the feed forward neural network (generally d_ffn = 2048)\n",
    "        :param dropout: probability that a neuron will be turned off during training (generally dropout = 0.1)\n",
    "        \"\"\"\n",
    "        super(TransformerDecoderLayer, self).__init__() \n",
    "\n",
    "        # Masked Multi-head Self-Attention\n",
    "        self.self_attn = MultiheadAttention(d_model, n_head, dropout=dropout)\n",
    "        # Masked Multi-head Self-Attention Layer Normalization\n",
    "        self.self_attn_norm = torch.nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Multi-head Cross-Attention\n",
    "        self.cross_attn = MultiheadAttention(d_model, n_head, dropout=dropout)\n",
    "        # Multi-head Cross-Attention Layer Normalization\n",
    "        self.cross_attn_norm = torch.nn.LayerNorm(d_model)\n",
    "        \n",
    "        \n",
    "        # Position-wise Feed Forward Neural Network\n",
    "        self.ffn = PositionwiseFF(d_model, d_ffn, dropout=dropout)\n",
    "        # Position-wise Feed Forward Neural Network Layer Normalization\n",
    "        self.ffn_norm = torch.nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass of the Decoder Layer\n",
    "        :param tgt: Embedding of the target sequence (shape: [seq_len, batch_size, d_model])\n",
    "        :param memory: Embedding of the source sequence (shape: [seq_len, batch_size, d_model])\n",
    "        :param tgt_mask: mask for the target sequence (shape: [seq_len, seq_len])\n",
    "        :param memory_mask: mask for the source sequence (shape: [seq_len, seq_len])\n",
    "        :return: Embedding of the target sequence (shape: [seq_len, batch_size, d_model])\n",
    "        \"\"\"\n",
    "        # Masked Multi-head Self-Attention\n",
    "        tgt2, masked_self_attn_weights = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask)\n",
    "        tgt = tgt + self.dropout(tgt2) # Adding\n",
    "        tgt = self.self_attn_norm(tgt) # Normalizing\n",
    "        \n",
    "        # Multi-head Cross-Attention\n",
    "        tgt2, cross_attn_weights = self.cross_attn(tgt, memory, memory, attn_mask=memory_mask)\n",
    "        tgt = tgt + self.dropout(tgt2) # Adding\n",
    "        tgt = self.cross_attn_norm(tgt) # Normalizing\n",
    "        \n",
    "        # Position-wise Feed Forward Neural Network\n",
    "        tgt2 = self.ffn(tgt)\n",
    "        tgt = tgt + self.dropout(tgt2)\n",
    "        tgt = self.ffn_norm(tgt)\n",
    "        \n",
    "        return tgt, masked_self_attn_weights\n",
    "    \n",
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_head, d_ffn, num_layers, max_seq_length, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Decoder Transformer Architecture\n",
    "        :param vocab_size: size of the vocabulary\n",
    "        :param d_model: dimension of the vectors flowing through the model (generally it is d_model = embed_size = 512)\n",
    "        :param n_head: number of heads in the multi-head attention models (generally n_head = 8) \n",
    "        :param d_ffn: dimension of the feed forward neural network (generally d_ffn = 2048)\n",
    "        :param num_layers: number of decoder layers (generally num_layers = 6)\n",
    "        :param max_seq_length: maximum sequence length\n",
    "        :param dropout: probability that a neuron will be turned off during training (generally dropout = 0.1)\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = torch.nn.ModuleList([TransformerDecoderLayer(d_model, n_head, d_ffn, dropout) for _ in range(num_layers)])\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        \n",
    "        # self.positional_encoding = PositionalEncoding(d_model, dropout=dropout, max_len=max_seq_length)\n",
    "        self.output_projection = torch.nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass of the Decoder\n",
    "        :param tgt: Embedding of the target sequence (shape: [seq_len, batch_size, d_model])\n",
    "        :param memory: Embedding of the source sequence (shape: [seq_len, batch_size, d_model])\n",
    "        :param tgt_mask: mask for the target sequence (shape: [seq_len, seq_len])\n",
    "        :param memory_mask: mask for the source sequence (shape: [seq_len, seq_len])\n",
    "        :return: Embedding of the target sequence (shape: [seq_len, batch_size, d_model])\n",
    "        \"\"\"\n",
    "        # tgt = self.positional_encoding(tgt)\n",
    "        for layer in self.layers:\n",
    "            tgt, masked_self_attn_weights, cross_attn_weights = layer(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask)\n",
    "        tgt = self.output_projection(tgt)\n",
    "        return tgt"
   ],
   "id": "7945e6a8e28e6279"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Image Captioning Model\n",
    "class ImageClassificationModel(torch.nn.Module):\n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder, image_size: int, vocab_size: int, max_seq_length: int,\n",
    "                    d_model: int=512, n_head: int=8, d_ffn: int=2048, dropout: float=0.1):\n",
    "        \"\"\"\n",
    "        Image Captioning Model\n",
    "        :param encoder: Encoder model\n",
    "        :param decoder: Decoder model\n",
    "        :param vocab_size: size of the vocabulary\n",
    "        \"\"\"\n",
    "        super(ImageClassificationModel, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.d_model = d_model\n",
    "        self.n_head = n_head\n",
    "        self.d_ffn = d_ffn\n",
    "        self.dropout = dropout\n",
    "        self.image_size = image_size\n",
    "        \n",
    "    def forward(self, image, tgt, tgt_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass of the Image Captioning Model\n",
    "        :param image: Tensor, shape [batch_size, 3, 224, 224]\n",
    "        :param tgt: Tensor, shape [seq_len, batch_size]\n",
    "        :param tgt_mask: mask for the target sequence (shape: [seq_len, seq_len])\n",
    "        :return: Tensor, shape [seq_len, batch_size, vocab_size]\n",
    "        \"\"\"\n",
    "        # Encode the image\n",
    "        image_features = self.encoder(image)\n",
    "        # Decode the image features\n",
    "        output = self.decoder(tgt, image_features, tgt_mask=tgt_mask)\n",
    "        return output\n",
    "        \n",
    "    def create_model(self, device, vocab_size, max_seq_length, d_model=512, n_head=8, d_ffn=2048, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Create the Image Captioning Model\n",
    "        :param device: device to run the model on\n",
    "        :param vocab_size: size of the source vocabulary\n",
    "        :param max_seq_length: size of the target vocabulary\n",
    "        :param d_model: dimension of the vectors flowing through the model (generally it is d_model = embed_size = 512)\n",
    "        :param n_head: number of heads in the multi-head attention models (generally n_head = 8)\n",
    "        :param d_ffn: dimension of the feed forward neural network (generally d_ffn = 2048)\n",
    "        :param dropout: probability that a neuron will be turned off during training (generally dropout = 0.1)\n",
    "        :return: Image Captioning Model\n",
    "        \"\"\"\n",
    "        # Create encoder\n",
    "        encoder = Encoder(embed_size=512)\n",
    "        # Create decoder\n",
    "        decoder = Decoder(vocab_size, d_model, n_head, d_ffn, num_layers=6, max_seq_length=max_seq_length, dropout=dropout)\n",
    "        # Create Image Captioning Model\n",
    "        model = ImageClassificationModel(encoder, decoder, image_size=224, vocab_size=vocab_size, max_seq_length=max_seq_length)\n",
    "        # Move the model to the device\n",
    "        model = model.to(device)\n",
    "        return model\n",
    "    \n",
    "        \n",
    "        \n"
   ],
   "id": "8522cc5cba24d41b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
